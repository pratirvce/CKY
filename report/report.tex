\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{qtree}
\usepackage{hyperref}
\usepackage{array}
\usepackage{makecell}

\newcolumntype{C}[1]{>{\centering\arraybackslash}m{#1}}

\title{Assignment 2: CKY Parsing}
\author{[Your Name]}
\date{}

\begin{document}
\maketitle

%% ============================================================
\section{Filling Out The Chart By Hand}
%% ============================================================

Given the CNF grammar and the sentence \textbf{``British left waffles on Falklands''}, we apply the CKY algorithm bottom-up. Each cell $[i,j]$ stores every non-terminal that can derive the span $w_i \ldots w_{j-1}$, together with backpointers indicating how it was built. When a non-terminal can be derived in more than one way, each derivation is listed separately.

\subsection{Grammar}

\begin{center}
\begin{tabular}{lll}
\toprule
\textbf{Binary Rules} & \textbf{Lexical Rules} \\
\midrule
S $\to$ NP\;VP   & NP $\to$ British \quad JJ $\to$ British \\
NP $\to$ JJ\;NP  & NP $\to$ left \quad\;\; VP $\to$ left \\
VP $\to$ VP\;NP   & NP $\to$ waffles \quad VP $\to$ waffles \\
VP $\to$ VP\;PP   & P $\to$ on \\
PP $\to$ P\;NP    & NP $\to$ Falklands \\
\bottomrule
\end{tabular}
\end{center}

\subsection{Completed Chart}

Table~\ref{tab:part1} shows the filled CKY chart. Subscripts denote backpointers: for a binary entry $A_{(B[i,k],\, C[k,j])}$, the non-terminal $A$ was built by combining $B$ from cell $[i,k]$ with $C$ from cell $[k,j]$.

\begin{table}[h]
\centering
\small
\setlength{\tabcolsep}{3pt}
\renewcommand{\arraystretch}{1.6}
\begin{tabular}{|c|C{2.2cm}|C{2.2cm}|C{2.2cm}|C{1.5cm}|C{2.2cm}|}
\hline
 & \textbf{British} & \textbf{left} & \textbf{waffles} & \textbf{on} & \textbf{Falklands} \\
\hline
$[0,\ast]$ &
  \makecell{NP \\ JJ} &
  \makecell{S$_{(\text{NP}[0,1],\,\text{VP}[1,2])}$ \\ NP$_{(\text{JJ}[0,1],\,\text{NP}[1,2])}$} &
  \makecell{S$_{(\text{NP}[0,1],\,\text{VP}[1,3])}$ \\ S$_{(\text{NP}[0,2],\,\text{VP}[2,3])}$} &
  --- &
  \makecell{S$_{(\text{NP}[0,1],\,\text{VP}[1,5])}$ \\ S$_{(\text{NP}[0,2],\,\text{VP}[2,5])}$} \\
\hline
$[1,\ast]$ & &
  \makecell{NP \\ VP} &
  \makecell{S$_{(\text{NP}[1,2],\,\text{VP}[2,3])}$ \\ VP$_{(\text{VP}[1,2],\,\text{NP}[2,3])}$} &
  --- &
  \makecell{S$_{(\text{NP}[1,2],\,\text{VP}[2,5])}$ \\ VP$_{(\text{VP}[1,3],\,\text{PP}[3,5])}$} \\
\hline
$[2,\ast]$ & & &
  \makecell{NP \\ VP} &
  --- &
  VP$_{(\text{VP}[2,3],\,\text{PP}[3,5])}$ \\
\hline
$[3,\ast]$ & & & &
  P &
  PP$_{(\text{P}[3,4],\,\text{NP}[4,5])}$ \\
\hline
$[4,\ast]$ & & & & &
  NP \\
\hline
\end{tabular}
\caption{CKY chart for ``British left waffles on Falklands'' with backpointers. Duplicate non-terminals with different derivations are listed separately.}
\label{tab:part1}
\end{table}

\subsection{Analysis}

Cell $[0,5]$ contains two S entries, corresponding to two distinct parse trees:

\begin{enumerate}[nosep]
  \item \textbf{Parse 1:} S $\to$ NP$[0,1]$ + VP$[1,5]$ --- ``British'' is the subject noun phrase; ``left waffles on Falklands'' is the verb phrase (left = verb).
  \item \textbf{Parse 2:} S $\to$ NP$[0,2]$ + VP$[2,5]$ --- ``British left'' is the subject (adjective + noun); ``waffles on Falklands'' is the verb phrase (waffles = verb).
\end{enumerate}

This structural ambiguity arises because both ``left'' and ``waffles'' can function as either nouns or verbs.

%% ============================================================
\section{Programming: CKY Algorithm}
%% ============================================================

\subsection{Implementation}

We implement the standard CKY algorithm following the pseudocode from the textbook. Each cell $[i,j]$ is a \textbf{set} of non-terminals, avoiding duplicate entries. The algorithm proceeds bottom-up: first filling the diagonal with lexical entries, then progressively filling longer spans by checking all binary rule combinations at each split point.

\subsection{Results}

Table~\ref{tab:part2} shows the parse table for the same sentence using sets.

\begin{table}[h]
\centering
\renewcommand{\arraystretch}{1.4}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
 & \textbf{British} & \textbf{left} & \textbf{waffles} & \textbf{on} & \textbf{Falklands} \\
\hline
$[0,\ast]$ & \{JJ, NP\} & \{NP, S\} & \{S\} & --- & \{S\} \\
\hline
$[1,\ast]$ & & \{NP, VP\} & \{S, VP\} & --- & \{S, VP\} \\
\hline
$[2,\ast]$ & & & \{NP, VP\} & --- & \{VP\} \\
\hline
$[3,\ast]$ & & & & \{P\} & \{PP\} \\
\hline
$[4,\ast]$ & & & & & \{NP\} \\
\hline
\end{tabular}
\caption{CKY parse table (set-based) for ``British left waffles on Falklands.''}
\label{tab:part2}
\end{table}

The start symbol S appears in cell $[0,5]$, confirming the sentence is grammatical. Compared with Part~1, each cell here contains at most one copy of each non-terminal, keeping the table compact and the algorithm polynomial.

%% ============================================================
\section{Programming: Weighted CKY Algorithm}
%% ============================================================

\subsection{Implementation}

We extend the CKY algorithm to handle a Probabilistic Context-Free Grammar (PCFG). Each cell stores a dictionary mapping each non-terminal to its \textbf{maximum probability} derivation (Viterbi) along with a backpointer for tree recovery.

For a binary rule $A \to BC$ with probability $P(A \to BC)$:
\[
  \text{score}(A, i, j) = \max_{k}\; P(A \to BC) \times \text{score}(B, i, k) \times \text{score}(C, k, j)
\]
We keep the maximizing split point $k$ and children $B$, $C$ as backpointers.

\subsection{PCFG}

\begin{center}
\begin{tabular}{ll@{\qquad}ll}
\toprule
\textbf{Rule} & \textbf{Prob} & \textbf{Rule} & \textbf{Prob} \\
\midrule
S $\to$ NP VP   & 1.0  & NP $\to$ NP PP       & 0.4  \\
PP $\to$ P NP   & 1.0  & NP $\to$ astronomers & 0.4  \\
VP $\to$ V NP   & 0.7  & NP $\to$ ears        & 0.18 \\
VP $\to$ VP PP  & 0.3  & NP $\to$ saw         & 0.04 \\
P $\to$ with    & 1.0  & NP $\to$ stars       & 0.18 \\
V $\to$ saw     & 1.0  & NP $\to$ telescopes  & 0.1  \\
\bottomrule
\end{tabular}
\end{center}

\subsection{Results}

Table~\ref{tab:part3} shows the Viterbi parse table for the sentence \textbf{``astronomers saw stars with ears''}.

\begin{table}[h]
\centering
\small
\renewcommand{\arraystretch}{1.4}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
 & \textbf{astro.} & \textbf{saw} & \textbf{stars} & \textbf{with} & \textbf{ears} \\
\hline
$[0,\ast]$ & NP: 0.4 & --- & S: 0.0504 & --- & S: 0.003629 \\
\hline
$[1,\ast]$ & & \makecell{V: 1.0 \\ NP: 0.04} & VP: 0.126 & --- & VP: 0.009072 \\
\hline
$[2,\ast]$ & & & NP: 0.18 & --- & NP: 0.01296 \\
\hline
$[3,\ast]$ & & & & P: 1.0 & PP: 0.18 \\
\hline
$[4,\ast]$ & & & & & NP: 0.18 \\
\hline
\end{tabular}
\caption{Viterbi CKY parse table for ``astronomers saw stars with ears.''}
\label{tab:part3}
\end{table}

\subsubsection{Most Probable Parse Tree}

The most probable parse has probability \textbf{0.0036288}:

\Tree[.S [.NP astronomers ] [.VP [.V saw ] [.NP [.NP stars ] [.PP [.P with ] [.NP ears ] ] ] ] ]

\medskip

This corresponds to the \textbf{NP-attachment} reading: ``with ears'' modifies ``stars'' (i.e., the astronomer saw stars that have ears). The probability breaks down as:

\[
  \underbrace{1.0}_{S \to \text{NP VP}} \times
  \underbrace{0.4}_{\text{NP} \to \text{astro.}} \times
  \underbrace{0.7}_{\text{VP} \to \text{V NP}} \times
  \underbrace{1.0}_{V \to \text{saw}} \times
  \underbrace{0.4}_{\text{NP} \to \text{NP PP}} \times
  \underbrace{0.18}_{\text{NP} \to \text{stars}} \times
  \underbrace{1.0}_{\text{PP} \to \text{P NP}} \times
  \underbrace{1.0}_{P \to \text{with}} \times
  \underbrace{0.18}_{\text{NP} \to \text{ears}}
  = 0.0036288
\]

The alternative VP-attachment parse (``with ears'' modifies ``saw'') has probability $0.0027216$, which is lower because VP $\to$ VP PP carries probability 0.3 versus VP $\to$ V NP at 0.7.

%% ============================================================
\section{Programming: Sum Over All Parse Trees}
%% ============================================================

\subsection{Implementation}

We modify the weighted CKY to compute the \textbf{inside probability}: instead of taking the maximum over split points and rules, we \textbf{sum} contributions. This implements the inside algorithm:
\[
  \text{inside}(A, i, j) = \sum_{A \to BC} \sum_{k=i+1}^{j-1} P(A \to BC) \times \text{inside}(B, i, k) \times \text{inside}(C, k, j)
\]

The total sentence probability is $\text{inside}(\text{S}, 0, n)$, marginalizing over all parse trees.

\subsection{Results}

Table~\ref{tab:part4} shows the inside-probability parse table.

\begin{table}[h]
\centering
\small
\renewcommand{\arraystretch}{1.4}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
 & \textbf{astro.} & \textbf{saw} & \textbf{stars} & \textbf{with} & \textbf{ears} \\
\hline
$[0,\ast]$ & NP: 0.4 & --- & S: 0.0504 & --- & S: 0.006350 \\
\hline
$[1,\ast]$ & & \makecell{V: 1.0 \\ NP: 0.04} & VP: 0.126 & --- & VP: 0.015876 \\
\hline
$[2,\ast]$ & & & NP: 0.18 & --- & NP: 0.01296 \\
\hline
$[3,\ast]$ & & & & P: 1.0 & PP: 0.18 \\
\hline
$[4,\ast]$ & & & & & NP: 0.18 \\
\hline
\end{tabular}
\caption{Inside-probability CKY table for ``astronomers saw stars with ears.''}
\label{tab:part4}
\end{table}

The key difference from the Viterbi table appears in cell $[1,5]$:
\begin{itemize}[nosep]
  \item \textbf{Viterbi} (Part~3): VP in $[1,5]$ = $\max(0.009072, 0.006804)$ = 0.009072
  \item \textbf{Sum} (Part~4): VP in $[1,5]$ = $0.009072 + 0.006804$ = 0.015876
\end{itemize}

The two derivations contributing to VP in $[1,5]$ are:
\begin{enumerate}[nosep]
  \item VP $\to$ V NP at $k{=}2$: $0.7 \times 1.0 \times 0.01296 = 0.009072$ (NP-attachment)
  \item VP $\to$ VP PP at $k{=}3$: $0.3 \times 0.126 \times 0.18 = 0.006804$ (VP-attachment)
\end{enumerate}

\medskip
\noindent\fbox{\parbox{0.95\linewidth}{%
\textbf{Total sentence probability} (summed over all parse trees):
\[
  P(\text{``astronomers saw stars with ears''}) = \mathbf{0.0063504}
\]
This equals the sum of the two individual tree probabilities: $0.0036288 + 0.0027216 = 0.0063504$.
}}

\end{document}
